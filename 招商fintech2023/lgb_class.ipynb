{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error, log_loss\n",
    "pd.options.display.precision = 15\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "#-------------------------------get data and label-------------------#\n",
    "train = pd.read_csv('/data/train_base.csv')\n",
    "test = pd.read_csv('/data/testa_base.csv')\n",
    "train_fea = pd.read_csv('train_trx_fea.csv')\n",
    "test_fea = pd.read_csv('test_trx_fea.csv')\n",
    "# train_trx_emb = pd.read_csv('train_trx_operation_emebdding.csv')\n",
    "# test_trx_emb = pd.read_csv('test_trx_trx_operation_emebdding.csv')\n",
    "train_fea_view = pd.read_csv('train_view_fea.csv')\n",
    "test_fea_view = pd.read_csv('test_view_fea.csv')\n",
    "view_emb = pd.read_csv('all_view_emb.csv')\n",
    "#w2v_view_emb = pd.read_csv('all_view_w2v_emb.csv')\n",
    "#trx_emb = pd.read_csv('all_trx_tfidf_emb.csv')\n",
    "\n",
    "\n",
    "train = pd.merge(train, train_fea, on=['cust_wid'],how = 'left')\n",
    "test = pd.merge(test, test_fea, on=['cust_wid'],how = 'left')\n",
    "\n",
    "train = pd.merge(train, train_fea_view, on=['cust_wid'],how = 'left')\n",
    "test = pd.merge(test, test_fea_view, on=['cust_wid'],how = 'left')\n",
    "\n",
    "# train = pd.merge(train, train_trx_emb, on=['cust_wid'],how = 'left')\n",
    "# test = pd.merge(test, test_trx_emb, on=['cust_wid'],how = 'left')\n",
    "\n",
    "train = pd.merge(train, view_emb, on=['cust_wid'],how = 'left')\n",
    "test = pd.merge(test, view_emb, on=['cust_wid'],how = 'left')\n",
    "\n",
    "# train = pd.merge(train, w2v_view_emb, on=['cust_wid'],how = 'left')\n",
    "# test = pd.merge(test, w2v_view_emb, on=['cust_wid'],how = 'left')\n",
    "\n",
    "train = train.dropna(subset = ['age','gdr_cd'])\n",
    "print(train.shape, test.shape)\n",
    "train['label'][train.label>=1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------some functional---------------------------------#\n",
    "# F2-score\n",
    "def f2_score(y_true, y_pred):\n",
    "    return 'f2', fbeta_score(np.round(y_true), np.array(y_pred.label), beta=2), True\n",
    "\n",
    "\n",
    "def pkl_save(filename,file):\n",
    "    output = open(filename, 'wb')\n",
    "    pickle.dump(file, output)\n",
    "    output.close()\n",
    "\n",
    "def pkl_load(filename):\n",
    "    pkl_file = open(filename, 'rb')\n",
    "    file = pickle.load(pkl_file) \n",
    "    pkl_file.close()\n",
    "    return file\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "    \n",
    "def fast_auc(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse = 0\n",
    "    auc = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n):\n",
    "        y_i = y_true[i]\n",
    "        nfalse += (1 - y_i)\n",
    "        auc += y_i * nfalse\n",
    "    auc /= (nfalse * (n - nfalse))\n",
    "    return auc\n",
    "\n",
    "\n",
    "def eval_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast auc eval function for lgb.\n",
    "    \"\"\"\n",
    "    return 'auc', fast_auc(y_true, y_pred), True\n",
    "\n",
    "\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "    \n",
    "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n",
    "    \"\"\"\n",
    "    A function to train a variety of classification models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    n_splits = folds.n_splits if splits is None else n_folds\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
    "                        'catboost_metric_name': 'AUC',\n",
    "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
    "                    }\n",
    "    \n",
    "    result_dict = {}\n",
    "    if averaging == 'usual':\n",
    "        # out-of-fold predictions on train data\n",
    "        oof = np.zeros((len(X), 1))\n",
    "\n",
    "        # averaged predictions on train data\n",
    "        prediction = np.zeros((len(X_test), 1))\n",
    "        \n",
    "    elif averaging == 'rank':\n",
    "        # out-of-fold predictions on train data\n",
    "        oof = np.zeros((len(X), 1))\n",
    "\n",
    "        # averaged predictions on train data\n",
    "        prediction = np.zeros((len(X_test), 1))\n",
    "\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            pkl_save(\"models/model-{}.lgb\".format(fold_n),model)\n",
    "            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n",
    "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function='Logloss')\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        if averaging == 'usual':\n",
    "            \n",
    "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "            \n",
    "            prediction += y_pred.reshape(-1, 1)\n",
    "\n",
    "        elif averaging == 'rank':\n",
    "                                  \n",
    "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "                                  \n",
    "            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12))\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "            plt.title('LGB Features (avg over folds)')\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "            result_dict['top_columns'] = cols\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "#---------------------------------------------cat_cols process--------------------------------#\n",
    "cat_cols = ['gdr_cd','cty_cd'] \n",
    "continue_cols = []\n",
    "for col in train.columns:\n",
    "    if col not in ['cust_wid','label']+cat_cols:\n",
    "        continue_cols.append(col)\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in train.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))   \n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------prepare training data -----------------------------#\n",
    "n_fold = 5\n",
    "#folds = TimeSeriesSplit(n_splits=n_fold)\n",
    "folds = KFold(n_splits=5)\n",
    "\n",
    "#continue_cols+ ['CUST_UID','LABEL']+cat_cols+question_cols+large_cale_cols\n",
    "X = train.sort_values('cust_wid').drop(['label', 'cust_wid'], axis=1)\n",
    "y = train.sort_values('cust_wid')['label']\n",
    "X_test = test.drop(['cust_wid'], axis=1)\n",
    "\n",
    "def clean_inf_nan(df):\n",
    "    return df.replace([np.inf, -np.inf], np.nan)   \n",
    "\n",
    "# Cleaning infinite values to NaN\n",
    "X = clean_inf_nan(X)\n",
    "X_test = clean_inf_nan(X_test )\n",
    "\n",
    "\n",
    "print(len(X.columns))\n",
    "print(X.shape)\n",
    "\n",
    "averag = 'usual'#'rank'#\n",
    "model_name = 'lgb'\n",
    "params = {'num_leaves': 63,#256,#\n",
    "          'min_child_samples': 79,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,#7,#15\n",
    "          'learning_rate': 0.03,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 3,\n",
    "          \"subsample\": 0.75,#0.9\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'binary_logloss',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 0.9,\n",
    "          #'categorical_feature': cat_cols\n",
    "         }\n",
    "\n",
    "result_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n",
    "                                                      verbose=50, early_stopping_rounds=20, n_estimators=5000, averaging=averag, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pkl_save(filename,file):\n",
    "    output = open(filename, 'wb')\n",
    "    pickle.dump(file, output)\n",
    "    output.close()\n",
    "\n",
    "def pkl_load(filename):\n",
    "    pkl_file = open(filename, 'rb')\n",
    "    file = pickle.load(pkl_file) \n",
    "    pkl_file.close()\n",
    "    return file\n",
    "\n",
    "\n",
    "def run_test(X_test,X):\n",
    "    result_dict = {}\n",
    "    columns_test = X.columns \n",
    "    print(X.columns)\n",
    "    n_splits = 5\n",
    "    X_test = X_test[columns_test]\n",
    "\n",
    "    models = [pkl_load(\"models/model-{}.lgb\".format(idx)) for idx in range(n_splits)]\n",
    "    prediction = np.zeros((len(X_test), 1))\n",
    "    for idx,model in enumerate(models):\n",
    "        y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n",
    "        prediction += y_pred.reshape(-1, 1)\n",
    "    \n",
    "    prediction /= n_splits\n",
    "    result_dict['prediction'] = prediction\n",
    "    return result_dict\n",
    "\n",
    "result_dict_lgb = run_test(X,X)\n",
    "result_dict_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = train.sort_values('cust_wid')[['cust_wid','label']]\n",
    "eval_data['pre'] = result_dict_lgb['prediction']\n",
    "eval_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def f2_metric(y_true, y_pred):\n",
    "    return 'f2', fbeta_score(y_true, y_pred,average='binary',  beta=2), True\n",
    "\n",
    "max_f2 = 0\n",
    "max_id = 0\n",
    "pred = result_dict_lgb['prediction']\n",
    "for i in np.arange(0,1,0.1):\n",
    "    \n",
    "    pred_binary = [1 if x >= i else 0 for x in pred]\n",
    "\n",
    "    # 计算F2-score\n",
    "    f2_score_val = f2_metric(np.array(eval_data['label']), pred_binary)\n",
    "    #print(f2_score_val)\n",
    "    if max_f2<= f2_score_val[1]:\n",
    "        max_f2 = f2_score_val[1]\n",
    "        max_id = i\n",
    "\n",
    "print(\"f2_score:\",max_f2,'max_id:',max_id)\n",
    "\n",
    "print(pred.shape,pred)\n",
    "print(roc_auc_score(np.array(eval_data['label']),pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
