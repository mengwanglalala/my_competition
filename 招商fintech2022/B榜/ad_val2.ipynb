{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 29) (12000, 28)\n",
      "LR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 64 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 45.27%\n",
      "RF...\n",
      "AUC: 72.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  74 out of 100 | elapsed:    0.5s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=64)]: Using backend ThreadingBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done  74 out of 100 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=64)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn import cross_validation as CV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "train = pd.read_csv('./data/original_train.csv')\n",
    "#testa = pd.read_csv('./data/original_test_A.csv')\n",
    "testb = pd.read_csv('./data/original_test_B.csv')\n",
    "# valid = pd.read_csv('./adversarial_set_data.csv')\n",
    "# valid = valid.drop( [ 'TARGET', 'predictions' ], axis = 1 )\n",
    "\n",
    "remain_col = ['AGN_CNT_RCT_12_MON',\n",
    " 'NB_RCT_3_MON_LGN_TMS_AGV',\n",
    " 'AGN_AGR_LATEST_AGN_AMT',\n",
    " 'MON_12_EXT_SAM_TRSF_IN_AMT',\n",
    " 'MON_12_EXT_SAM_TRSF_OUT_AMT',\n",
    " 'MON_12_EXT_SAM_NM_TRSF_OUT_CNT',\n",
    " 'MON_12_EXT_SAM_AMT',\n",
    " 'CUR_MON_EXT_SAM_CUST_TRSF_IN_AMT',\n",
    " 'CUR_MON_EXT_SAM_CUST_TRSF_OUT_AMT',\n",
    " 'MON_12_TRX_AMT_MAX_AMT_PCTT',\n",
    " 'MON_12_AGV_TRX_CNT',\n",
    " 'MON_12_ACM_ENTR_ACT_CNT',\n",
    " 'MON_12_AGV_ENTR_ACT_CNT',\n",
    " 'MON_12_ACM_LVE_ACT_CNT',\n",
    " 'MON_12_AGV_LVE_ACT_CNT',\n",
    " 'MON_6_50_UP_ENTR_ACT_CNT',\n",
    " 'MON_6_50_UP_LVE_ACT_CNT',\n",
    " 'MON_12_ACT_OUT_50_UP_CNT_PTY_QTY',\n",
    " 'MON_12_ACT_IN_50_UP_CNT_PTY_QTY',\n",
    " 'LAST_12_MON_COR_DPS_TM_PNT_BAL_PEAK_VAL',\n",
    " 'LAST_12_MON_COR_DPS_DAY_AVG_BAL',\n",
    " 'LAST_12_MON_DIF_NM_MON_AVG_TRX_AMT_NAV',\n",
    " 'LAST_12_MON_MON_AVG_TRX_AMT_NAV',\n",
    " 'REG_CPT',\n",
    " 'HLD_DMS_CCY_ACT_NBR',\n",
    " 'REG_DT',\n",
    " 'HLD_FGN_CCY_ACT_NBR']\n",
    "# 73['AGN_CNT_RCT_12_MON', 'COR_KEY_PROD_HLD_NBR', 'LAST_12_MON_COR_DPS_DAY_AVG_BAL', 'LAST_12_MON_COR_DPS_TM_PNT_BAL_PEAK_VAL', 'LAST_12_MON_DIF_NM_MON_AVG_TRX_AMT_NAV', 'LAST_12_MON_MON_AVG_TRX_AMT_NAV', 'MON_12_ACM_ENTR_ACT_CNT', 'MON_12_ACT_IN_50_UP_CNT_PTY_QTY', 'MON_12_AGV_ENTR_ACT_CNT', 'MON_12_EXT_SAM_AMT', 'MON_12_EXT_SAM_TRSF_OUT_AMT', 'MON_6_50_UP_ENTR_ACT_CNT', 'NB_RCT_3_MON_LGN_TMS_AGV', 'OPN_TM', 'REG_DT']\n",
    "# 76 ['COR_KEY_PROD_HLD_NBR', 'CUR_MON_EXT_SAM_CUST_TRSF_OUT_AMT', 'CUR_YEAR_COUNTER_ENCASH_CNT', 'EMP_NBR', 'HLD_DMS_CCY_ACT_NBR', 'HLD_FGN_CCY_ACT_NBR', 'LAST_12_MON_COR_DPS_DAY_AVG_BAL', 'LAST_12_MON_COR_DPS_TM_PNT_BAL_PEAK_VAL', 'LAST_12_MON_MON_AVG_TRX_AMT_NAV', 'MON_12_ACT_OUT_50_UP_CNT_PTY_QTY', 'MON_12_AGV_ENTR_ACT_CNT', 'MON_12_EXT_SAM_TRSF_IN_AMT', 'MON_6_50_UP_ENTR_ACT_CNT', 'NB_RCT_3_MON_LGN_TMS_AGV', 'REG_DT']\n",
    "# train = train.drop(need_drop_col + my_guess, axis=1)\n",
    "# test = test.drop(need_drop_col + my_guess, axis=1)\n",
    "train = train[['CUST_UID','LABEL'] + remain_col]\n",
    "test = testb[['CUST_UID'] + remain_col]\n",
    "#test = testb\n",
    "print(train.shape,test.shape)\n",
    "\n",
    "# import copy\n",
    "# train = copy.deepcopy(train)\n",
    "# test = copy.deepcopy(test)\n",
    "train['TARGET'] = 0\n",
    "test['TARGET'] = 1\n",
    "\n",
    "\n",
    "data = pd.concat(( train, test ))\n",
    "\n",
    "# shuffle\n",
    "data = data.iloc[ np.random.permutation(len( data )) ]\n",
    "data.reset_index( drop = True, inplace = True )\n",
    "cat_cols = ['MON_12_CUST_CNT_PTY_ID','AI_STAR_SCO','WTHR_OPN_ONL_ICO',\n",
    "'SHH_BCK','LGP_HLD_CARD_LVL','NB_CTC_HLD_IDV_AIO_CARD_SITU'] \n",
    "question_cols = ['REG_CPT','CUR_YEAR_MID_BUS_INC']\n",
    "large_cale_cols = ['ICO_CUR_MON_ACM_TRX_AMT',\n",
    "'COUNTER_CUR_YEAR_CNT_AMT',\n",
    "'PUB_TO_PRV_TRX_AMT_CUR_YEAR',\n",
    "'MON_12_EXT_SAM_TRSF_IN_AMT',\n",
    "'MON_12_EXT_SAM_TRSF_OUT_AMT',\n",
    "'MON_12_EXT_SAM_AMT',\n",
    "'CUR_MON_EXT_SAM_CUST_TRSF_IN_AMT',\n",
    "'CUR_MON_EXT_SAM_CUST_TRSF_OUT_AMT',\n",
    "'LAST_12_MON_COR_DPS_TM_PNT_BAL_PEAK_VAL',\n",
    "'LAST_12_MON_COR_DPS_DAY_AVG_BAL',\n",
    "'LAST_12_MON_DIF_NM_MON_AVG_TRX_AMT_NAV',\n",
    "'LAST_12_MON_MON_AVG_TRX_AMT_NAV'\n",
    "]\n",
    "\n",
    "large_offset  = ['AGN_CUR_YEAR_AMT',\n",
    "'AGN_CUR_YEAR_WAG_AMT',\n",
    "'PUB_TO_PRV_TRX_AMT_CUR_YEAR',\n",
    "'CUR_YEAR_MID_BUS_INC'\n",
    "]\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in data.columns:\n",
    "        data = data.drop( [ col] , axis = 1 )\n",
    "def clean_inf_nan(df):\n",
    "    return df.replace([np.inf, -np.inf], np.nan)   \n",
    "\n",
    "# Cleaning infinite values to NaN\n",
    "data = clean_inf_nan(data)\n",
    "data = data.fillna(-1)\n",
    "x = data.drop( [ 'TARGET', 'CUST_UID', 'LABEL' ] , axis = 1 )\n",
    "y = data.TARGET\n",
    "\n",
    "train_examples = len(data)-12000\n",
    "x_train = x[:train_examples]\n",
    "x_test = x[train_examples:]\n",
    "y_train = y[:train_examples]\n",
    "y_test = y[train_examples:]\n",
    "\n",
    "#\n",
    "\n",
    "print (\"LR...\")\n",
    "\n",
    "clf = LR()\n",
    "clf.fit( x_train, y_train )\n",
    "\n",
    "# predict\n",
    "\n",
    "p = clf.predict_proba( x_test )[:,1]\n",
    "auc = AUC( y_test, p )\n",
    "print (\"AUC: {:.2%}\".format( auc ))\n",
    "\n",
    "# AUC: 50.06%\n",
    "\n",
    "print (\"RF...\")\n",
    "\n",
    "clf = RF( n_estimators = 100, verbose = True, n_jobs = -1 )\n",
    "clf.fit( x_train, y_train )\n",
    "\n",
    "p = clf.predict_proba( x_test )[:,1]\n",
    "auc = AUC( y_test, p )\n",
    "print (\"AUC: {:.2%}\".format( auc ))\n",
    "\n",
    "# AUC: 50.28%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend ThreadingBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done  74 out of 100 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=64)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 53)\n",
      "                               CUST_UID  LABEL  AGN_CNT_RCT_12_MON  \\\n",
      "50394  77cacef0e32745fca4c9e21846d2a6c1    0.0                -1.0   \n",
      "47685  b397b06dd45e47c8b8374a7b967b49a1    0.0                42.0   \n",
      "50899  afed25b29193433ab4b835e501ba00a7    1.0              2872.0   \n",
      "47242  0d52778ec12944008627550ecfaf59c6    1.0                -1.0   \n",
      "43359  2b0bb2e8a3f64c8780e103c3e5fc7ef7    0.0                -1.0   \n",
      "\n",
      "       ICO_CUR_MON_ACM_TRX_TM  NB_RCT_3_MON_LGN_TMS_AGV  AGN_CUR_YEAR_AMT  \\\n",
      "50394                    -1.0                      -1.0              -1.0   \n",
      "47685                    62.0                     318.7          612008.0   \n",
      "50899                   332.0                    1042.0         5777058.3   \n",
      "47242                    42.0                     472.0              -1.0   \n",
      "43359                    12.0                      42.0              -1.0   \n",
      "\n",
      "       AGN_CUR_YEAR_WAG_AMT  AGN_AGR_LATEST_AGN_AMT  ICO_CUR_MON_ACM_TRX_AMT  \\\n",
      "50394                  -1.0                    -1.0                     -1.0   \n",
      "47685              612008.0                154701.0               12254701.0   \n",
      "50899                  -1.0                984315.9               42452273.9   \n",
      "47242                  -1.0                    -1.0                4051282.0   \n",
      "43359                  -1.0                    -1.0                2800002.0   \n",
      "\n",
      "       COUNTER_CUR_YEAR_CNT_AMT  ...       REG_CPT  SHH_BCK  \\\n",
      "50394                      -1.0  ...  1.000000e+09     12.0   \n",
      "47685                       2.0  ...  3.000000e+08     12.0   \n",
      "50899                       2.0  ...  5.000000e+07     12.0   \n",
      "47242                       2.0  ...  1.000000e+08     32.0   \n",
      "43359                       2.0  ... -1.000000e+00      2.0   \n",
      "\n",
      "       HLD_DMS_CCY_ACT_NBR   REG_DT  LGP_HLD_CARD_LVL   OPN_TM  \\\n",
      "50394                 32.0  2404.90                -1  1582.32   \n",
      "47685                 12.0  2508.45                 D   287.48   \n",
      "50899                 12.0   935.87                 C   744.26   \n",
      "47242                 12.0  1302.00                 C   934.26   \n",
      "43359                 12.0    -1.00                -1   866.84   \n",
      "\n",
      "       NB_CTC_HLD_IDV_AIO_CARD_SITU HLD_FGN_CCY_ACT_NBR  TARGET  predictions  \n",
      "50394                            -1                 2.0       0         0.71  \n",
      "47685                             D                 2.0       0         0.74  \n",
      "50899                             C                 2.0       0         0.75  \n",
      "47242                             C                 2.0       0         0.77  \n",
      "43359                            -1                 2.0       0         0.82  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "# meta_train = pd.DataFrame()\n",
    "# meta_test = pd.DataFrame()\n",
    "# meta_train['predictions'] = pd.Series(result_dict_lgb['oof'].reshape(-1))#.rank()\n",
    "#seperate train rows which have been misclassified as test and use them as validation\n",
    "import copy\n",
    "train2 = copy.deepcopy(data)#.sort_values('CUST_UID')\n",
    "train2[\"predictions\"] = pd.Series(clf.predict_proba(data.drop( [ 'TARGET', 'CUST_UID', 'LABEL' ] + cat_cols+large_offset, axis = 1 ))[:,1].reshape(-1))\n",
    "predictions_argsort = train2['predictions'].argsort()\n",
    "#print(predictions_argsort)\n",
    "train_sorted = train2.iloc[predictions_argsort]\n",
    "# print(train_sorted.tail())\n",
    "#select only trains set because we need to find train rows which have been misclassified as test set and use them for validation\n",
    "train_sorted = train_sorted.loc[train_sorted.TARGET == 0]\n",
    "\n",
    "#Why did I chose 0.7 as thereshold? just a hunch, but you should try different thresholds i.e 0.6, 0.8 and see the difference in validation score and please report back. :) \n",
    "train_as_test = train_sorted.loc[train_sorted.predictions > 0.7]\n",
    "print(train_as_test.shape)\n",
    "print(train_as_test.head())\n",
    "# train_as_test.to_csv('adversarial_set_data.csv', index=False)\n",
    "# #save the indices of the misclassified train rows to use as validation set\n",
    "# adversarial_set_ids = train_as_test.index.values\n",
    "# adversarial_set = pd.DataFrame(adversarial_set_ids, columns=['adversial_set_ids'])\n",
    "# #save adversarial set index\n",
    "# adversarial_set.to_csv('adversarial_set_ids.csv', index=False)\n",
    "# print(adversarial_set.shape)\n",
    "# adversarial_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2e8f04d65c62c49dbc0f2adda76bbd103f82c3255fc54c7e3855bd92e2af645"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
