{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from deepctr_torch.inputs import SparseFeat, get_feature_names, DenseFeat\n",
    "from deepctr_torch.models import DeepFM\n",
    "import deepctr_torch\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_pickle(f'../temp_data/df_train_v6.pickle')\n",
    "val = pd.read_pickle('../temp_data/df_valid_v6.pickle')\n",
    "test = pd.read_pickle('../temp_data/df_test_v6.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycol = 'is_finish'\n",
    "\n",
    "feats = ['userid','videoid','tag', 'videoid_cnt', 'videoid_is_like_sum',\n",
    "       'videoid_is_favourite_sum', 'videoid_is_share_sum',\n",
    "       'videoid_is_finish_sum', 'videoid_is_like_mean',\n",
    "       'videoid_is_favourite_mean', 'videoid_is_share_mean',\n",
    "       'videoid_is_finish_mean', 'userid_cnt', 'userid_is_like_sum',\n",
    "       'userid_is_favourite_sum', 'userid_is_share_sum',\n",
    "       'userid_is_finish_sum', 'userid_is_like_mean',\n",
    "       'userid_is_favourite_mean', 'userid_is_share_mean',\n",
    "       'userid_is_finish_mean']\n",
    "train = train[feats+['is_finish']]\n",
    "val = val[feats+['is_finish']]\n",
    "test = test[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = ['userid','videoid','tag']#'userid',\n",
    "dense_features = [i for i in train.columns if i not in ['is_finish','tag','userid','videoid','flag','is_like', 'is_share', 'is_favourite']]\n",
    "train[dense_features] = train[dense_features].fillna(0.0, )\n",
    "val[dense_features] = val[dense_features].fillna(0.0, )\n",
    "test[dense_features] = test[dense_features].fillna(0.0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(dense_features):\n",
    "    train[i] = (train[i] - train[i].min())/(train[i].max() - train[i].min())\n",
    "    val[i] = (val[i] - val[i].min())/(val[i].max() - val[i].min())\n",
    "    test[i] = (test[i] - test[i].min())/(test[i].max() - test[i].min())\n",
    "fixlen_feature_columns = [SparseFeat('userid',  vocabulary_size=500000, embedding_dim=8),\n",
    "                         SparseFeat('videoid', vocabulary_size=500000, embedding_dim=8),\n",
    "                         SparseFeat('tag', vocabulary_size=train['tag'].nunique(), embedding_dim=8)] + [DenseFeat(feat, 1, )for feat in dense_features]\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "val_model_input = {name: val[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adagrad\n",
    "model = deepctr_torch.models.wdl.WDL(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns,\n",
    "                   task='binary',\n",
    "                   l2_reg_embedding=1e-5, device='cpu')#,dnn_hidden_units=(256, 256, 256)\n",
    "model.compile(Adagrad(model.parameters(),0.1), \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\", \"auc\"], )\n",
    "for epoch in tqdm(range(2)):\n",
    "    model.fit(train_model_input,train['is_finish'].values,batch_size=1024,epochs=1,verbose=2)\n",
    "    pred = model.predict(val_model_input, 4096)\n",
    "    print('epoch ', epoch, 'log_loss: ', log_loss(val['is_finish'],pred))\n",
    "    \n",
    "\n",
    "predict = model.predict(test_model_input, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../init_data/toUser/test/test.csv')\n",
    "sub['is_finish'] = predict\n",
    "sub[['ID','is_finish']].to_csv('../temp_data/nn.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
