{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "import catboost as ctb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_pickle(f'../temp_data/df_train_v6.pickle')\n",
    "df_valid = pd.read_pickle('../temp_data/df_valid_v6.pickle')\n",
    "df_test = pd.read_pickle('../temp_data/df_test_v6.pickle')\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    for col in tqdm(df.columns):\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(\n",
    "                        np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(\n",
    "                        np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(\n",
    "                        np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(\n",
    "                        np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(\n",
    "                        np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(\n",
    "                        np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_valid = reduce_mem_usage(df_valid)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "print(df_train.shape, df_valid.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {df_train.isnull().any().sum()} columns in train dataset with missing values.')\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique() <= 1]\n",
    "one_value_cols_test = [col for col in df_test.columns if df_test[col].nunique() <= 1]\n",
    "print(one_value_cols,one_value_cols_test)\n",
    "many_null_cols = [col for col in df_train.columns if df_train[col].isnull().sum() / df_train.shape[0] > 0.9]\n",
    "many_null_cols_test = [col for col in df_test.columns if df_test[col].isnull().sum() / df_test.shape[0] > 0.9]\n",
    "print(many_null_cols,many_null_cols_test)\n",
    "big_top_value_cols = [col for col in df_train.columns if df_train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "big_top_value_cols_test = [col for col in df_test.columns if df_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "print(big_top_value_cols,big_top_value_cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_finish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [c for c in df_train.columns if c not in ['ID','is_like',\n",
    " 'is_favourite',\n",
    " 'is_share','seq_no_rank',\n",
    " 'is_finish']+big_top_value_cols+many_null_cols+one_value_cols]\n",
    "ycol = 'is_finish'\n",
    "\n",
    "feats = ['tag', 'videoid_cnt', 'videoid_is_like_sum',\n",
    "       'videoid_is_favourite_sum', 'videoid_is_share_sum',\n",
    "       'videoid_is_finish_sum', 'videoid_is_like_mean',\n",
    "       'videoid_is_favourite_mean', 'videoid_is_share_mean',\n",
    "       'videoid_is_finish_mean', 'userid_cnt', 'userid_is_like_sum',\n",
    "       'userid_is_favourite_sum', 'userid_is_share_sum',\n",
    "       'userid_is_finish_sum', 'userid_is_like_mean',\n",
    "       'userid_is_favourite_mean', 'userid_is_share_mean',\n",
    "       'userid_is_finish_mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\n",
    "    'task_type': 'GPU',\n",
    "    'learning_rate': 0.1,\n",
    "    'loss_function': 'Logloss',\n",
    "    'iterations': 100000,  # 10000,\n",
    "    'random_seed': 2022,\n",
    "    'max_depth': 6,\n",
    "    'reg_lambda': 0.05,\n",
    "    'early_stopping_rounds': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(**cat_params)\n",
    "model.fit(\n",
    "    df_train[feats], df_train[ycol],\n",
    "    eval_set=(df_valid[feats], df_valid[ycol]),\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid[ycol + '_score'] = model.predict_proba(df_valid[feats])[:, 1]\n",
    "val_log = log_loss(df_valid[ycol], df_valid[ycol + '_score'])\n",
    "print('val log_loss: ', val_log)\n",
    "predict = model.predict_proba(df_test[feats])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.DataFrame({\n",
    "    'column': feats,\n",
    "    'importance': model.feature_importances_,\n",
    "})\n",
    "df_importance.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../init_data/toUser/test/test.csv')\n",
    "\n",
    "sub[ycol] = predict\n",
    "sub[['ID',ycol]].to_csv('../temp_data/cat.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sub)\n",
    "sub.is_finish.describe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
